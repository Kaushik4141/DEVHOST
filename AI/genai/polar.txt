

===== https://docs.pola.rs/polars-cloud/ =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting started
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Key Features of Polars Cloud
Install Polars Cloud
Example query
Sign up today and start your 30 day trial
Cloud availability
Introducing Polars Cloud
DataFrame implementations always differed from SQL and databases. SQL could run anywhere from
embedded databases to massive data warehouses. Yet, DataFrame users have been forced to choose
between a solution for local work or solutions geared towards distributed computing, each with their
own APIs and limitations.
Polars is bridging this gap withPolars Cloud. Build on top of the popular open source project,
Polars Cloud enables you to write DataFrame code once and run it anywhere. The distributed engine
available with Polars Cloud allows to scale your Polars queries beyond a single machine.
Key Features of Polars Cloud
Unified DataFrame Experience: Run a Polars query seamlessly on your local machine and at scale
  with our new distributed engine. All from the same API.
Serverless Compute: Effortlessly start compute resources without managing infrastructure with
  options to execute queries on both CPU and GPU (coming soon).
Any Environment: Start a remote query from a notebook on your machine, Airflow DAG, AWS
  Lambda, or your server. Get the flexibility to embed Polars Cloud in any environment.
Install Polars Cloud
Simply extend the capabilities of Polars with:
pipinstallpolarspolars_cloud
pipinstallpolarspolars_cloud
Example query
To run your query in the cloud, simply write Polars queries like you are used to, but callLazyFrame.remote()to indicate that the query should be run remotely.
LazyFrame.remote()
ComputeContext·LazyFrameRemoteimportpolarsasplimportpolars_cloudaspcctx=pc.ComputeContext(workspace="your-workspace",cpus=16,memory=64)query=(pl.scan_parquet("s3://my-dataset/").group_by("l_returnflag","l_linestatus").agg(avg_price=pl.mean("l_extendedprice"),avg_disc=pl.mean("l_discount"),count_order=pl.len(),))(query.remote(context=ctx).sink_parquet("s3://my-dst/"))
ComputeContext
LazyFrameRemote
importpolarsasplimportpolars_cloudaspcctx=pc.ComputeContext(workspace="your-workspace",cpus=16,memory=64)query=(pl.scan_parquet("s3://my-dataset/").group_by("l_returnflag","l_linestatus").agg(avg_price=pl.mean("l_extendedprice"),avg_disc=pl.mean("l_discount"),count_order=pl.len(),))(query.remote(context=ctx).sink_parquet("s3://my-dst/"))
importpolarsasplimportpolars_cloudaspcctx=pc.ComputeContext(workspace="your-workspace",cpus=16,memory=64)query=(pl.scan_parquet("s3://my-dataset/").group_by("l_returnflag","l_linestatus").agg(avg_price=pl.mean("l_extendedprice"),avg_disc=pl.mean("l_discount"),count_order=pl.len(),))(query.remote(context=ctx).sink_parquet("s3://my-dst/"))
Sign up today and start your 30 day trial
Polars Cloud is available to try with a 30 day free trial. You can sign up oncloud.pola.rsto get started.
Cloud availability
Polars Cloud is available on AWS. Other cloud providers and on-premise solutions are on the roadmap
and will become available in the upcoming months.



===== https://docs.pola.rs/polars-cloud/#introducing-polars-cloud =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting started
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Key Features of Polars Cloud
Install Polars Cloud
Example query
Sign up today and start your 30 day trial
Cloud availability
Introducing Polars Cloud
DataFrame implementations always differed from SQL and databases. SQL could run anywhere from
embedded databases to massive data warehouses. Yet, DataFrame users have been forced to choose
between a solution for local work or solutions geared towards distributed computing, each with their
own APIs and limitations.
Polars is bridging this gap withPolars Cloud. Build on top of the popular open source project,
Polars Cloud enables you to write DataFrame code once and run it anywhere. The distributed engine
available with Polars Cloud allows to scale your Polars queries beyond a single machine.
Key Features of Polars Cloud
Unified DataFrame Experience: Run a Polars query seamlessly on your local machine and at scale
  with our new distributed engine. All from the same API.
Serverless Compute: Effortlessly start compute resources without managing infrastructure with
  options to execute queries on both CPU and GPU (coming soon).
Any Environment: Start a remote query from a notebook on your machine, Airflow DAG, AWS
  Lambda, or your server. Get the flexibility to embed Polars Cloud in any environment.
Install Polars Cloud
Simply extend the capabilities of Polars with:
pipinstallpolarspolars_cloud
pipinstallpolarspolars_cloud
Example query
To run your query in the cloud, simply write Polars queries like you are used to, but callLazyFrame.remote()to indicate that the query should be run remotely.
LazyFrame.remote()
ComputeContext·LazyFrameRemoteimportpolarsasplimportpolars_cloudaspcctx=pc.ComputeContext(workspace="your-workspace",cpus=16,memory=64)query=(pl.scan_parquet("s3://my-dataset/").group_by("l_returnflag","l_linestatus").agg(avg_price=pl.mean("l_extendedprice"),avg_disc=pl.mean("l_discount"),count_order=pl.len(),))(query.remote(context=ctx).sink_parquet("s3://my-dst/"))
ComputeContext
LazyFrameRemote
importpolarsasplimportpolars_cloudaspcctx=pc.ComputeContext(workspace="your-workspace",cpus=16,memory=64)query=(pl.scan_parquet("s3://my-dataset/").group_by("l_returnflag","l_linestatus").agg(avg_price=pl.mean("l_extendedprice"),avg_disc=pl.mean("l_discount"),count_order=pl.len(),))(query.remote(context=ctx).sink_parquet("s3://my-dst/"))
importpolarsasplimportpolars_cloudaspcctx=pc.ComputeContext(workspace="your-workspace",cpus=16,memory=64)query=(pl.scan_parquet("s3://my-dataset/").group_by("l_returnflag","l_linestatus").agg(avg_price=pl.mean("l_extendedprice"),avg_disc=pl.mean("l_discount"),count_order=pl.len(),))(query.remote(context=ctx).sink_parquet("s3://my-dst/"))
Sign up today and start your 30 day trial
Polars Cloud is available to try with a 30 day free trial. You can sign up oncloud.pola.rsto get started.
Cloud availability
Polars Cloud is available on AWS. Other cloud providers and on-premise solutions are on the roadmap
and will become available in the upcoming months.



===== https://docs.pola.rs/user-guide/getting-started/ =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#getting-started =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#installing-polars =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#reading-writing =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#expressions-and-contexts =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#select =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#with_columns =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#filter =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#group_by =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#more-complex-queries =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#combining-dataframes =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#joining-dataframes =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/getting-started/#concatenating-dataframes =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframesInstallationConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting startedGetting startedTable of contentsInstalling PolarsReading & writingExpressions and contextsselectwith_columnsfiltergroup_byMore complex queriesCombining dataframesJoining dataframesConcatenating dataframes
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Installation
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Installing Polars
Reading & writing
Expressions and contextsselectwith_columnsfiltergroup_byMore complex queries
select
with_columns
filter
group_by
More complex queries
Combining dataframesJoining dataframesConcatenating dataframes
Joining dataframes
Concatenating dataframes
Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to thenext chapter about installation options.
Installing Polars
pipinstallpolars
pipinstallpolars
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
DataFrameimportpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrame
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
importpolarsasplimportdatetimeasdtdf=pl.DataFrame({"name":["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate":[dt.date(1997,1,10),dt.date(1985,2,15),dt.date(1983,3,22),dt.date(1981,4,30),],"weight":[57.9,72.5,53.6,83.1],# (kg)"height":[1.56,1.77,1.65,1.75],# (m)})print(df)
DataFrameusechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
DataFrame
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
usechrono::prelude::*;usepolars::prelude::*;letmutdf:DataFrame=df!("name"=>["Alice Archer","Ben Brown","Chloe Cooper","Daniel Donovan"],"birthdate"=>[NaiveDate::from_ymd_opt(1997,1,10).unwrap(),NaiveDate::from_ymd_opt(1985,2,15).unwrap(),NaiveDate::from_ymd_opt(1983,3,22).unwrap(),NaiveDate::from_ymd_opt(1981,4,30).unwrap(),],"weight"=>[57.9,72.5,53.6,83.1],// (kg)"height"=>[1.56,1.77,1.65,1.75],// (m)).unwrap();println!("{df}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
In the example below we write the dataframe to a csv file calledoutput.csv. After that, we read
it back usingread_csvand then print the result for inspection.
output.csv
read_csv
read_csv·write_csvdf.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
read_csv
write_csv
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
df.write_csv("docs/assets/data/output.csv")df_csv=pl.read_csv("docs/assets/data/output.csv",try_parse_dates=True)print(df_csv)
CsvReader·CsvWriter·Available on feature csvusestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
CsvReader
CsvWriter
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
usestd::fs::File;letmutfile=File::create("docs/assets/data/output.csv").expect("could not create file");CsvWriter::new(&mutfile).include_header(true).with_separator(b',').finish(&mutdf)?;letdf_csv=CsvReadOptions::default().with_has_header(true).with_parse_options(CsvParseOptions::default().with_try_parse_dates(true)).try_into_reader_with_file_path(Some("docs/assets/data/output.csv".into()))?.finish()?;println!("{df_csv}");
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (4, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
For more examples on the CSV file format and other data formats, see theIO sectionof the user guide.
Expressions and contexts
Expressionsare one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
pl.col("weight")/(pl.col("height")**2)
pl.col("weight")/(pl.col("height")**2)
As you might be able to guess, this expression takes the column named “weight” and divides its
values by the square of the values in the column “height”, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polarscontextthat the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
select
select
with_columns
with_columns
filter
filter
group_by
group_by
For a moredetailed exploration of expressions and contexts see the respective user guide section.
select
select
The contextselectallows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
select
select·alias·dt namespaceresult=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select
alias
dt namespace
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
result=df.select(pl.col("name"),pl.col("birthdate").dt.year().alias("birth_year"),(pl.col("weight")/(pl.col("height")**2)).alias("bmi"),)print(result)
select·alias·dt namespace·Available on feature temporalletresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
select
alias
dt namespace
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
shape: (4, 3)
┌────────────────┬────────────┬───────────┐
│ name           ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---       │
│ str            ┆ i32        ┆ f64       │
╞════════════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴───────────┘
Polars also supports a feature called “expression expansion”, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns “weight” and “height” with a single expression. When using expression expansion you can
use.name.suffixto add a suffix to the names of the original columns:
.name.suffix
select·alias·name namespaceresult=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select
alias
name namespace
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
result=df.select(pl.col("name"),(pl.col("weight","height")*0.95).round(2).name.suffix("-5%"),)print(result)
select·alias·name namespace·Available on feature lazyletresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
select
alias
name namespace
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().select([col("name"),(cols(["weight","height"]).as_expr()*lit(0.95)).round(2,RoundMode::default()).name().suffix("-5%"),]).collect()?;println!("{result}");
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
shape: (4, 3)
┌────────────────┬───────────┬───────────┐
│ name           ┆ weight-5% ┆ height-5% │
│ ---            ┆ ---       ┆ ---       │
│ str            ┆ f64       ┆ f64       │
╞════════════════╪═══════════╪═══════════╡
│ Alice Archer   ┆ 55.0      ┆ 1.48      │
│ Ben Brown      ┆ 68.88     ┆ 1.68      │
│ Chloe Cooper   ┆ 50.92     ┆ 1.57      │
│ Daniel Donovan ┆ 78.94     ┆ 1.66      │
└────────────────┴───────────┴───────────┘
You can check other sections of the user guide to learn more aboutbasic operationsorcolumn selections in expression expansion.
with_columns
with_columns
The contextwith_columnsis very similar to the contextselectbutwith_columnsadds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions insidewith_columns:
with_columns
select
with_columns
with_columns
with_columnsresult=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columns
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
result=df.with_columns(birth_year=pl.col("birthdate").dt.year(),bmi=pl.col("weight")/(pl.col("height")**2),)print(result)
with_columnsletresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
with_columns
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([col("birthdate").dt().year().alias("birth_year"),(col("weight")/col("height").pow(2)).alias("bmi"),]).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────────┬───────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ birth_year ┆ bmi       │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---        ┆ ---       │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ i32        ┆ f64       │
╞════════════════╪════════════╪════════╪════════╪════════════╪═══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ 1997       ┆ 23.791913 │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ 1985       ┆ 23.141498 │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ 1983       ┆ 19.687787 │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ 1981       ┆ 27.134694 │
└────────────────┴────────────┴────────┴────────┴────────────┴───────────┘
In the example above we also decided to use named expressions instead of the methodaliasto
specify the names of the new columns. Other contexts likeselectandgroup_byalso accept named
expressions.
alias
select
group_by
filter
filter
The contextfilterallows us to create a second dataframe with a subset of the rows of the
original one:
filter
filter·dt namespaceresult=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter
dt namespace
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
result=df.filter(pl.col("birthdate").dt.year()<1990)print(result)
filter·dt namespace·Available on feature temporalletresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
filter
dt namespace
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").dt().year().lt(lit(1990))).collect()?;println!("{result}");
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
shape: (3, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
└────────────────┴────────────┴────────┴────────┘
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with&:
&
filter·is_betweenresult=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter
is_between
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
result=df.filter(pl.col("birthdate").is_between(dt.date(1982,12,31),dt.date(1996,1,1)),pl.col("height")>1.7,)print(result)
filter·is_between·Available on feature is_betweenletresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
filter
is_between
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
letresult=df.clone().lazy().filter(col("birthdate").is_between(lit(NaiveDate::from_ymd_opt(1982,12,31).unwrap()),lit(NaiveDate::from_ymd_opt(1996,1,1).unwrap()),ClosedInterval::Both,).and(col("height").gt(lit(1.7))),).collect()?;println!("{result}");
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
shape: (1, 4)
┌───────────┬────────────┬────────┬────────┐
│ name      ┆ birthdate  ┆ weight ┆ height │
│ ---       ┆ ---        ┆ ---    ┆ ---    │
│ str       ┆ date       ┆ f64    ┆ f64    │
╞═══════════╪════════════╪════════╪════════╡
│ Ben Brown ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
└───────────┴────────────┴────────┴────────┘
group_by
group_by
The contextgroup_bycan be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
group_by
group_by·alias·dt namespaceresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by
alias
dt namespace
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).len()print(result)
group_by·alias·dt namespace·Available on feature temporal// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
group_by
alias
dt namespace
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
// Use `group_by_stable` if you want the Python behaviour of `maintain_order=True`.letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len()]).collect()?;println!("{result}");
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
shape: (2, 2)
┌────────┬─────┐
│ decade ┆ len │
│ ---    ┆ --- │
│ i32    ┆ u64 │
╞════════╪═════╡
│ 1990   ┆ 1   │
│ 1980   ┆ 3   │
└────────┴─────┘
The keyword argumentmaintain_orderforces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
maintain_order
After using the contextgroup_bywe can useaggto compute aggregations over the resulting
groups:
group_by
agg
group_by·aggresult=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by
agg
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
result=df.group_by((pl.col("birthdate").dt.year()//10*10).alias("decade"),maintain_order=True,).agg(pl.len().alias("sample_size"),pl.col("weight").mean().round(2).alias("avg_weight"),pl.col("height").max().alias("tallest"),)print(result)
group_by·aggletresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
group_by
agg
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().group_by([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade")]).agg([len().alias("sample_size"),col("weight").mean().round(2,RoundMode::default()).alias("avg_weight"),col("height").max().alias("tallest"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
shape: (2, 4)
┌────────┬─────────────┬────────────┬─────────┐
│ decade ┆ sample_size ┆ avg_weight ┆ tallest │
│ ---    ┆ ---         ┆ ---        ┆ ---     │
│ i32    ┆ u64         ┆ f64        ┆ f64     │
╞════════╪═════════════╪════════════╪═════════╡
│ 1990   ┆ 1           ┆ 57.9       ┆ 1.56    │
│ 1980   ┆ 3           ┆ 69.73      ┆ 1.77    │
└────────┴─────────────┴────────────┴─────────┘
More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
group_by·agg·select·with_columns·str namespace·list namespaceresult=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by
agg
select
with_columns
str namespace
list namespace
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
result=(df.with_columns((pl.col("birthdate").dt.year()//10*10).alias("decade"),pl.col("name").str.split(by=" ").list.first(),).select(pl.all().exclude("birthdate"),).group_by(pl.col("decade"),maintain_order=True,).agg(pl.col("name"),pl.col("weight","height").mean().round(2).name.prefix("avg_"),))print(result)
group_by·agg·select·with_columns·str namespace·list namespace·Available on feature stringsletresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
group_by
agg
select
with_columns
str namespace
list namespace
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
letresult=df.clone().lazy().with_columns([(col("birthdate").dt().year()/lit(10)*lit(10)).alias("decade"),col("name").str().split(lit(" ")).list().first(),]).select([all().exclude_cols(["birthdate"]).as_expr()]).group_by([col("decade")]).agg([col("name"),cols(["weight","height"]).as_expr().mean().round(2,RoundMode::default()).name().prefix("avg_"),]).collect()?;println!("{result}");
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
shape: (2, 4)
┌────────┬────────────────────────────┬────────────┬────────────┐
│ decade ┆ name                       ┆ avg_weight ┆ avg_height │
│ ---    ┆ ---                        ┆ ---        ┆ ---        │
│ i32    ┆ list[str]                  ┆ f64        ┆ f64        │
╞════════╪════════════════════════════╪════════════╪════════════╡
│ 1990   ┆ ["Alice"]                  ┆ 57.9       ┆ 1.56       │
│ 1980   ┆ ["Ben", "Chloe", "Daniel"] ┆ 69.73      ┆ 1.72       │
└────────┴────────────────────────────┴────────────┴────────────┘
Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
joindf2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
join
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
df2=pl.DataFrame({"name":["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent":[True,False,False,False],"siblings":[1,2,3,4],})print(df.join(df2,on="name",how="left"))
joinletdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
join
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
letdf2:DataFrame=df!("name"=>["Ben Brown","Daniel Donovan","Alice Archer","Chloe Cooper"],"parent"=>[true,false,false,false],"siblings"=>[1,2,3,4],).unwrap();letresult=df.clone().lazy().join(df2.lazy(),[col("name")],[col("name")],JoinArgs::new(JoinType::Left),).collect()?;println!("{result}");
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
shape: (4, 6)
┌────────────────┬────────────┬────────┬────────┬────────┬──────────┐
│ name           ┆ birthdate  ┆ weight ┆ height ┆ parent ┆ siblings │
│ ---            ┆ ---        ┆ ---    ┆ ---    ┆ ---    ┆ ---      │
│ str            ┆ date       ┆ f64    ┆ f64    ┆ bool   ┆ i64      │
╞════════════════╪════════════╪════════╪════════╪════════╪══════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   ┆ false  ┆ 3        │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   ┆ true   ┆ 1        │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   ┆ false  ┆ 4        │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   ┆ false  ┆ 2        │
└────────────────┴────────────┴────────┴────────┴────────┴──────────┘
Polars provides many different join algorithms that you can learn about in thejoins section of the user guide.
Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
concatdf3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concat
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
df3=pl.DataFrame({"name":["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate":[dt.date(1977,5,10),dt.date(1975,6,23),dt.date(1973,7,22),dt.date(1971,8,3),],"weight":[67.9,72.5,57.6,93.1],# (kg)"height":[1.76,1.6,1.66,1.8],# (m)})print(pl.concat([df,df3],how="vertical"))
concatletdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
concat
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
letdf3:DataFrame=df!("name"=>["Ethan Edwards","Fiona Foster","Grace Gibson","Henry Harris"],"birthdate"=>[NaiveDate::from_ymd_opt(1977,5,10).unwrap(),NaiveDate::from_ymd_opt(1975,6,23).unwrap(),NaiveDate::from_ymd_opt(1973,7,22).unwrap(),NaiveDate::from_ymd_opt(1971,8,3).unwrap(),],"weight"=>[67.9,72.5,57.6,93.1],// (kg)"height"=>[1.76,1.6,1.66,1.8],// (m)).unwrap();letresult=concat([df.clone().lazy(),df3.lazy()],UnionArgs::default())?.collect()?;println!("{result}");
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
shape: (8, 4)
┌────────────────┬────────────┬────────┬────────┐
│ name           ┆ birthdate  ┆ weight ┆ height │
│ ---            ┆ ---        ┆ ---    ┆ ---    │
│ str            ┆ date       ┆ f64    ┆ f64    │
╞════════════════╪════════════╪════════╪════════╡
│ Alice Archer   ┆ 1997-01-10 ┆ 57.9   ┆ 1.56   │
│ Ben Brown      ┆ 1985-02-15 ┆ 72.5   ┆ 1.77   │
│ Chloe Cooper   ┆ 1983-03-22 ┆ 53.6   ┆ 1.65   │
│ Daniel Donovan ┆ 1981-04-30 ┆ 83.1   ┆ 1.75   │
│ Ethan Edwards  ┆ 1977-05-10 ┆ 67.9   ┆ 1.76   │
│ Fiona Foster   ┆ 1975-06-23 ┆ 72.5   ┆ 1.6    │
│ Grace Gibson   ┆ 1973-07-22 ┆ 57.6   ┆ 1.66   │
│ Henry Harris   ┆ 1971-08-03 ┆ 93.1   ┆ 1.8    │
└────────────────┴────────────┴────────┴────────┘
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in theconcatenations section of the user guide.



===== https://docs.pola.rs/user-guide/installation/ =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting started
InstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
Installation
Polars is a library and installation is as simple as invoking the package manager of the
corresponding programming language.
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Big Index
By default, Polars dataframes are limited to\(2^{32}\)rows (~4.3 billion). Increase this limit to\(2^{64}\)(~18 quintillion) by enabling the big index extension:
pipinstallpolars-u64-idx
pipinstallpolars-u64-idx
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
Legacy CPU
To install Polars for Python on an old CPU withoutAVXsupport, run:
pipinstallpolars-lts-cpu
pipinstallpolars-lts-cpu
Importing
To use the library, simply import it into your project:
importpolarsaspl
importpolarsaspl
usepolars::prelude::*;
usepolars::prelude::*;
Feature flags
By using the above command you install the core of Polars onto your system. However, depending on
your use case, you might want to install the optional dependencies as well. These are made optional
to minimize the footprint. The flags are different depending on the programming language. Throughout
the user guide we will mention when a functionality used requires an additional dependency.
Python
# For example
pip install 'polars[numpy,fsspec]'
# For example
pip install 'polars[numpy,fsspec]'
Note
SeeGPU supportfor more detailed instructions and
prerequisites.
plot
style
Rust
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
The opt-in features are:
Additional data types:dtype-datedtype-datetimedtype-timedtype-durationdtype-i8dtype-i16dtype-u8dtype-u16dtype-categoricaldtype-struct
dtype-date
dtype-date
dtype-datetime
dtype-datetime
dtype-time
dtype-time
dtype-duration
dtype-duration
dtype-i8
dtype-i8
dtype-i16
dtype-i16
dtype-u8
dtype-u8
dtype-u16
dtype-u16
dtype-categorical
dtype-categorical
dtype-struct
dtype-struct
lazy- Lazy API:regex- Use regexes in column selection.dot_diagram- Create dot diagrams from lazy logical plans.
lazy
regex- Use regexes in column selection.
regex
dot_diagram- Create dot diagrams from lazy logical plans.
dot_diagram
sql- Pass SQL queries to Polars.
sql
streaming- Be able to process datasets that are larger than RAM.
streaming
random- Generate arrays with randomly sampled values
random
ndarray- Convert fromDataFrametondarray
ndarray
DataFrame
ndarray
temporal- Conversions betweenChronoand Polars for temporal data types
temporal
timezones- Activate timezone support.
timezones
strings- Extra string utilities forStringChunked:string_pad- forpad_start,pad_end,zfill.string_to_integer- forparse_int.
strings
StringChunked
string_pad- forpad_start,pad_end,zfill.
string_pad
pad_start
pad_end
zfill
string_to_integer- forparse_int.
string_to_integer
parse_int
object- Support for generic ChunkedArrays calledObjectChunked<T>(generic overT).
  These are downcastable from Series through theAnytrait.
object
ObjectChunked<T>
T
Performance related:nightly- Several nightly only features such as SIMD and specialization.performant- more fast paths, slower compile times.bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.cse- Activate common subplan elimination optimization.
nightly- Several nightly only features such as SIMD and specialization.
nightly
performant- more fast paths, slower compile times.
performant
bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.
bigidx
u64
cse- Activate common subplan elimination optimization.
cse
IO related:serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.parquet- Read Apache Parquet format.json- JSON serialization.ipc- Arrow's IPC format serialization.decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:gzipzlibzstd
serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde
serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde-lazy
parquet- Read Apache Parquet format.
parquet
json- JSON serialization.
json
ipc- Arrow's IPC format serialization.
ipc
decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:
decompress
gzip
zlib
zstd
Dataframe operations:dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.sort_multiple- Allow sorting a dataframe on multiple columns.rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.join_asof- Join ASOF, to join on nearest keys instead of exact equality match.cross_join- Create the Cartesian product of two dataframes.semi_anti_join- SEMI and ANTI joins.row_hash- Utility to hash dataframe rows toUInt64Chunked.diagonal_concat- Diagonal concatenation thereby combining different schemas.dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.partition_by- Split into multiple dataframes partitioned by groups.
dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.
dynamic_group_by
sort_multiple- Allow sorting a dataframe on multiple columns.
sort_multiple
rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.
rows
dataframes
pivot
transpose
join_asof- Join ASOF, to join on nearest keys instead of exact equality match.
join_asof
cross_join- Create the Cartesian product of two dataframes.
cross_join
semi_anti_join- SEMI and ANTI joins.
semi_anti_join
row_hash- Utility to hash dataframe rows toUInt64Chunked.
row_hash
UInt64Chunked
diagonal_concat- Diagonal concatenation thereby combining different schemas.
diagonal_concat
dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.
dataframe_arithmetic
partition_by- Split into multiple dataframes partitioned by groups.
partition_by
Series/expression operations:is_in- Check for membership in Series.zip_with- Zip twoSeries/ChunkedArrays.round_series- round underlying float types of series.repeat_by- Repeat element in an array a number of times specified by another array.is_first_distinct- Check if element is first unique value.is_last_distinct- Check if element is last unique value.checked_arithmetic- checked arithmetic returningNoneon invalid operations.dot_product- Dot/inner product on series and expressions.concat_str- Concatenate string data in linear time.reinterpret- Utility to reinterpret bits to signed/unsigned.take_opt_iter- Take from a series withIterator<Item=Option<usize>>.mode- Return the most frequently occurring value(s).cum_agg-cum_sum,cum_min, andcum_max, aggregations.rolling_window- rolling window functions, likerolling_mean.interpolate- Interpolate intermediateNonevalues.extract_jsonpath-Runjsonpathqueries onStringChunked.list- List utils:list_gather- take sublist by multiple indices.rank- Ranking algorithms.moment- Kurtosis and skew statistics.ewma- Exponential moving average windows.abs- Get absolute values of series.arange- Range operation on series.product- Compute the product of a series.diff-diffoperation.pct_change- Compute change percentages.unique_counts- Count unique values in expressions.log- Logarithms for series.list_to_struct- ConvertListtoStructdata types.list_count- Count elements in lists.list_eval- Apply expressions over list elements.cumulative_eval- Apply expressions over cumulatively increasing windows.arg_where- Get indices where condition holds.search_sorted- Find indices where elements should be inserted to maintain order.offset_by- Add an offset to dates that take months and leap years into account.trigonometry- Trigonometric functions.sign- Compute the element-wise sign of a series.propagate_nans-NaN-propagating min/max aggregations.
is_in- Check for membership in Series.
is_in
zip_with- Zip twoSeries/ChunkedArrays.
zip_with
Series
ChunkedArray
round_series- round underlying float types of series.
round_series
repeat_by- Repeat element in an array a number of times specified by another array.
repeat_by
is_first_distinct- Check if element is first unique value.
is_first_distinct
is_last_distinct- Check if element is last unique value.
is_last_distinct
checked_arithmetic- checked arithmetic returningNoneon invalid operations.
checked_arithmetic
None
dot_product- Dot/inner product on series and expressions.
dot_product
concat_str- Concatenate string data in linear time.
concat_str
reinterpret- Utility to reinterpret bits to signed/unsigned.
reinterpret
take_opt_iter- Take from a series withIterator<Item=Option<usize>>.
take_opt_iter
Iterator<Item=Option<usize>>
mode- Return the most frequently occurring value(s).
mode
cum_agg-cum_sum,cum_min, andcum_max, aggregations.
cum_agg
cum_sum
cum_min
cum_max
rolling_window- rolling window functions, likerolling_mean.
rolling_window
rolling_mean
interpolate- Interpolate intermediateNonevalues.
interpolate
None
extract_jsonpath-Runjsonpathqueries onStringChunked.
extract_jsonpath
jsonpath
StringChunked
list- List utils:
list
list_gather- take sublist by multiple indices.
list_gather
rank- Ranking algorithms.
rank
moment- Kurtosis and skew statistics.
moment
ewma- Exponential moving average windows.
ewma
abs- Get absolute values of series.
abs
arange- Range operation on series.
arange
product- Compute the product of a series.
product
diff-diffoperation.
diff
diff
pct_change- Compute change percentages.
pct_change
unique_counts- Count unique values in expressions.
unique_counts
log- Logarithms for series.
log
list_to_struct- ConvertListtoStructdata types.
list_to_struct
List
Struct
list_count- Count elements in lists.
list_count
list_eval- Apply expressions over list elements.
list_eval
cumulative_eval- Apply expressions over cumulatively increasing windows.
cumulative_eval
arg_where- Get indices where condition holds.
arg_where
search_sorted- Find indices where elements should be inserted to maintain order.
search_sorted
offset_by- Add an offset to dates that take months and leap years into account.
offset_by
trigonometry- Trigonometric functions.
trigonometry
sign- Compute the element-wise sign of a series.
sign
propagate_nans-NaN-propagating min/max aggregations.
propagate_nans
NaN
Dataframe pretty printing:fmt- Activate dataframe formatting.
fmt- Activate dataframe formatting.
fmt
Only needed if you are on Windows.↩
Only needed if you are on Windows.↩



===== https://docs.pola.rs/user-guide/installation/#installation =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting started
InstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
Installation
Polars is a library and installation is as simple as invoking the package manager of the
corresponding programming language.
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Big Index
By default, Polars dataframes are limited to\(2^{32}\)rows (~4.3 billion). Increase this limit to\(2^{64}\)(~18 quintillion) by enabling the big index extension:
pipinstallpolars-u64-idx
pipinstallpolars-u64-idx
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
Legacy CPU
To install Polars for Python on an old CPU withoutAVXsupport, run:
pipinstallpolars-lts-cpu
pipinstallpolars-lts-cpu
Importing
To use the library, simply import it into your project:
importpolarsaspl
importpolarsaspl
usepolars::prelude::*;
usepolars::prelude::*;
Feature flags
By using the above command you install the core of Polars onto your system. However, depending on
your use case, you might want to install the optional dependencies as well. These are made optional
to minimize the footprint. The flags are different depending on the programming language. Throughout
the user guide we will mention when a functionality used requires an additional dependency.
Python
# For example
pip install 'polars[numpy,fsspec]'
# For example
pip install 'polars[numpy,fsspec]'
Note
SeeGPU supportfor more detailed instructions and
prerequisites.
plot
style
Rust
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
The opt-in features are:
Additional data types:dtype-datedtype-datetimedtype-timedtype-durationdtype-i8dtype-i16dtype-u8dtype-u16dtype-categoricaldtype-struct
dtype-date
dtype-date
dtype-datetime
dtype-datetime
dtype-time
dtype-time
dtype-duration
dtype-duration
dtype-i8
dtype-i8
dtype-i16
dtype-i16
dtype-u8
dtype-u8
dtype-u16
dtype-u16
dtype-categorical
dtype-categorical
dtype-struct
dtype-struct
lazy- Lazy API:regex- Use regexes in column selection.dot_diagram- Create dot diagrams from lazy logical plans.
lazy
regex- Use regexes in column selection.
regex
dot_diagram- Create dot diagrams from lazy logical plans.
dot_diagram
sql- Pass SQL queries to Polars.
sql
streaming- Be able to process datasets that are larger than RAM.
streaming
random- Generate arrays with randomly sampled values
random
ndarray- Convert fromDataFrametondarray
ndarray
DataFrame
ndarray
temporal- Conversions betweenChronoand Polars for temporal data types
temporal
timezones- Activate timezone support.
timezones
strings- Extra string utilities forStringChunked:string_pad- forpad_start,pad_end,zfill.string_to_integer- forparse_int.
strings
StringChunked
string_pad- forpad_start,pad_end,zfill.
string_pad
pad_start
pad_end
zfill
string_to_integer- forparse_int.
string_to_integer
parse_int
object- Support for generic ChunkedArrays calledObjectChunked<T>(generic overT).
  These are downcastable from Series through theAnytrait.
object
ObjectChunked<T>
T
Performance related:nightly- Several nightly only features such as SIMD and specialization.performant- more fast paths, slower compile times.bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.cse- Activate common subplan elimination optimization.
nightly- Several nightly only features such as SIMD and specialization.
nightly
performant- more fast paths, slower compile times.
performant
bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.
bigidx
u64
cse- Activate common subplan elimination optimization.
cse
IO related:serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.parquet- Read Apache Parquet format.json- JSON serialization.ipc- Arrow's IPC format serialization.decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:gzipzlibzstd
serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde
serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde-lazy
parquet- Read Apache Parquet format.
parquet
json- JSON serialization.
json
ipc- Arrow's IPC format serialization.
ipc
decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:
decompress
gzip
zlib
zstd
Dataframe operations:dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.sort_multiple- Allow sorting a dataframe on multiple columns.rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.join_asof- Join ASOF, to join on nearest keys instead of exact equality match.cross_join- Create the Cartesian product of two dataframes.semi_anti_join- SEMI and ANTI joins.row_hash- Utility to hash dataframe rows toUInt64Chunked.diagonal_concat- Diagonal concatenation thereby combining different schemas.dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.partition_by- Split into multiple dataframes partitioned by groups.
dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.
dynamic_group_by
sort_multiple- Allow sorting a dataframe on multiple columns.
sort_multiple
rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.
rows
dataframes
pivot
transpose
join_asof- Join ASOF, to join on nearest keys instead of exact equality match.
join_asof
cross_join- Create the Cartesian product of two dataframes.
cross_join
semi_anti_join- SEMI and ANTI joins.
semi_anti_join
row_hash- Utility to hash dataframe rows toUInt64Chunked.
row_hash
UInt64Chunked
diagonal_concat- Diagonal concatenation thereby combining different schemas.
diagonal_concat
dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.
dataframe_arithmetic
partition_by- Split into multiple dataframes partitioned by groups.
partition_by
Series/expression operations:is_in- Check for membership in Series.zip_with- Zip twoSeries/ChunkedArrays.round_series- round underlying float types of series.repeat_by- Repeat element in an array a number of times specified by another array.is_first_distinct- Check if element is first unique value.is_last_distinct- Check if element is last unique value.checked_arithmetic- checked arithmetic returningNoneon invalid operations.dot_product- Dot/inner product on series and expressions.concat_str- Concatenate string data in linear time.reinterpret- Utility to reinterpret bits to signed/unsigned.take_opt_iter- Take from a series withIterator<Item=Option<usize>>.mode- Return the most frequently occurring value(s).cum_agg-cum_sum,cum_min, andcum_max, aggregations.rolling_window- rolling window functions, likerolling_mean.interpolate- Interpolate intermediateNonevalues.extract_jsonpath-Runjsonpathqueries onStringChunked.list- List utils:list_gather- take sublist by multiple indices.rank- Ranking algorithms.moment- Kurtosis and skew statistics.ewma- Exponential moving average windows.abs- Get absolute values of series.arange- Range operation on series.product- Compute the product of a series.diff-diffoperation.pct_change- Compute change percentages.unique_counts- Count unique values in expressions.log- Logarithms for series.list_to_struct- ConvertListtoStructdata types.list_count- Count elements in lists.list_eval- Apply expressions over list elements.cumulative_eval- Apply expressions over cumulatively increasing windows.arg_where- Get indices where condition holds.search_sorted- Find indices where elements should be inserted to maintain order.offset_by- Add an offset to dates that take months and leap years into account.trigonometry- Trigonometric functions.sign- Compute the element-wise sign of a series.propagate_nans-NaN-propagating min/max aggregations.
is_in- Check for membership in Series.
is_in
zip_with- Zip twoSeries/ChunkedArrays.
zip_with
Series
ChunkedArray
round_series- round underlying float types of series.
round_series
repeat_by- Repeat element in an array a number of times specified by another array.
repeat_by
is_first_distinct- Check if element is first unique value.
is_first_distinct
is_last_distinct- Check if element is last unique value.
is_last_distinct
checked_arithmetic- checked arithmetic returningNoneon invalid operations.
checked_arithmetic
None
dot_product- Dot/inner product on series and expressions.
dot_product
concat_str- Concatenate string data in linear time.
concat_str
reinterpret- Utility to reinterpret bits to signed/unsigned.
reinterpret
take_opt_iter- Take from a series withIterator<Item=Option<usize>>.
take_opt_iter
Iterator<Item=Option<usize>>
mode- Return the most frequently occurring value(s).
mode
cum_agg-cum_sum,cum_min, andcum_max, aggregations.
cum_agg
cum_sum
cum_min
cum_max
rolling_window- rolling window functions, likerolling_mean.
rolling_window
rolling_mean
interpolate- Interpolate intermediateNonevalues.
interpolate
None
extract_jsonpath-Runjsonpathqueries onStringChunked.
extract_jsonpath
jsonpath
StringChunked
list- List utils:
list
list_gather- take sublist by multiple indices.
list_gather
rank- Ranking algorithms.
rank
moment- Kurtosis and skew statistics.
moment
ewma- Exponential moving average windows.
ewma
abs- Get absolute values of series.
abs
arange- Range operation on series.
arange
product- Compute the product of a series.
product
diff-diffoperation.
diff
diff
pct_change- Compute change percentages.
pct_change
unique_counts- Count unique values in expressions.
unique_counts
log- Logarithms for series.
log
list_to_struct- ConvertListtoStructdata types.
list_to_struct
List
Struct
list_count- Count elements in lists.
list_count
list_eval- Apply expressions over list elements.
list_eval
cumulative_eval- Apply expressions over cumulatively increasing windows.
cumulative_eval
arg_where- Get indices where condition holds.
arg_where
search_sorted- Find indices where elements should be inserted to maintain order.
search_sorted
offset_by- Add an offset to dates that take months and leap years into account.
offset_by
trigonometry- Trigonometric functions.
trigonometry
sign- Compute the element-wise sign of a series.
sign
propagate_nans-NaN-propagating min/max aggregations.
propagate_nans
NaN
Dataframe pretty printing:fmt- Activate dataframe formatting.
fmt- Activate dataframe formatting.
fmt
Only needed if you are on Windows.↩
Only needed if you are on Windows.↩



===== https://docs.pola.rs/user-guide/installation/#big-index =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting started
InstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
Installation
Polars is a library and installation is as simple as invoking the package manager of the
corresponding programming language.
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Big Index
By default, Polars dataframes are limited to\(2^{32}\)rows (~4.3 billion). Increase this limit to\(2^{64}\)(~18 quintillion) by enabling the big index extension:
pipinstallpolars-u64-idx
pipinstallpolars-u64-idx
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
Legacy CPU
To install Polars for Python on an old CPU withoutAVXsupport, run:
pipinstallpolars-lts-cpu
pipinstallpolars-lts-cpu
Importing
To use the library, simply import it into your project:
importpolarsaspl
importpolarsaspl
usepolars::prelude::*;
usepolars::prelude::*;
Feature flags
By using the above command you install the core of Polars onto your system. However, depending on
your use case, you might want to install the optional dependencies as well. These are made optional
to minimize the footprint. The flags are different depending on the programming language. Throughout
the user guide we will mention when a functionality used requires an additional dependency.
Python
# For example
pip install 'polars[numpy,fsspec]'
# For example
pip install 'polars[numpy,fsspec]'
Note
SeeGPU supportfor more detailed instructions and
prerequisites.
plot
style
Rust
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
The opt-in features are:
Additional data types:dtype-datedtype-datetimedtype-timedtype-durationdtype-i8dtype-i16dtype-u8dtype-u16dtype-categoricaldtype-struct
dtype-date
dtype-date
dtype-datetime
dtype-datetime
dtype-time
dtype-time
dtype-duration
dtype-duration
dtype-i8
dtype-i8
dtype-i16
dtype-i16
dtype-u8
dtype-u8
dtype-u16
dtype-u16
dtype-categorical
dtype-categorical
dtype-struct
dtype-struct
lazy- Lazy API:regex- Use regexes in column selection.dot_diagram- Create dot diagrams from lazy logical plans.
lazy
regex- Use regexes in column selection.
regex
dot_diagram- Create dot diagrams from lazy logical plans.
dot_diagram
sql- Pass SQL queries to Polars.
sql
streaming- Be able to process datasets that are larger than RAM.
streaming
random- Generate arrays with randomly sampled values
random
ndarray- Convert fromDataFrametondarray
ndarray
DataFrame
ndarray
temporal- Conversions betweenChronoand Polars for temporal data types
temporal
timezones- Activate timezone support.
timezones
strings- Extra string utilities forStringChunked:string_pad- forpad_start,pad_end,zfill.string_to_integer- forparse_int.
strings
StringChunked
string_pad- forpad_start,pad_end,zfill.
string_pad
pad_start
pad_end
zfill
string_to_integer- forparse_int.
string_to_integer
parse_int
object- Support for generic ChunkedArrays calledObjectChunked<T>(generic overT).
  These are downcastable from Series through theAnytrait.
object
ObjectChunked<T>
T
Performance related:nightly- Several nightly only features such as SIMD and specialization.performant- more fast paths, slower compile times.bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.cse- Activate common subplan elimination optimization.
nightly- Several nightly only features such as SIMD and specialization.
nightly
performant- more fast paths, slower compile times.
performant
bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.
bigidx
u64
cse- Activate common subplan elimination optimization.
cse
IO related:serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.parquet- Read Apache Parquet format.json- JSON serialization.ipc- Arrow's IPC format serialization.decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:gzipzlibzstd
serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde
serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde-lazy
parquet- Read Apache Parquet format.
parquet
json- JSON serialization.
json
ipc- Arrow's IPC format serialization.
ipc
decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:
decompress
gzip
zlib
zstd
Dataframe operations:dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.sort_multiple- Allow sorting a dataframe on multiple columns.rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.join_asof- Join ASOF, to join on nearest keys instead of exact equality match.cross_join- Create the Cartesian product of two dataframes.semi_anti_join- SEMI and ANTI joins.row_hash- Utility to hash dataframe rows toUInt64Chunked.diagonal_concat- Diagonal concatenation thereby combining different schemas.dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.partition_by- Split into multiple dataframes partitioned by groups.
dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.
dynamic_group_by
sort_multiple- Allow sorting a dataframe on multiple columns.
sort_multiple
rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.
rows
dataframes
pivot
transpose
join_asof- Join ASOF, to join on nearest keys instead of exact equality match.
join_asof
cross_join- Create the Cartesian product of two dataframes.
cross_join
semi_anti_join- SEMI and ANTI joins.
semi_anti_join
row_hash- Utility to hash dataframe rows toUInt64Chunked.
row_hash
UInt64Chunked
diagonal_concat- Diagonal concatenation thereby combining different schemas.
diagonal_concat
dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.
dataframe_arithmetic
partition_by- Split into multiple dataframes partitioned by groups.
partition_by
Series/expression operations:is_in- Check for membership in Series.zip_with- Zip twoSeries/ChunkedArrays.round_series- round underlying float types of series.repeat_by- Repeat element in an array a number of times specified by another array.is_first_distinct- Check if element is first unique value.is_last_distinct- Check if element is last unique value.checked_arithmetic- checked arithmetic returningNoneon invalid operations.dot_product- Dot/inner product on series and expressions.concat_str- Concatenate string data in linear time.reinterpret- Utility to reinterpret bits to signed/unsigned.take_opt_iter- Take from a series withIterator<Item=Option<usize>>.mode- Return the most frequently occurring value(s).cum_agg-cum_sum,cum_min, andcum_max, aggregations.rolling_window- rolling window functions, likerolling_mean.interpolate- Interpolate intermediateNonevalues.extract_jsonpath-Runjsonpathqueries onStringChunked.list- List utils:list_gather- take sublist by multiple indices.rank- Ranking algorithms.moment- Kurtosis and skew statistics.ewma- Exponential moving average windows.abs- Get absolute values of series.arange- Range operation on series.product- Compute the product of a series.diff-diffoperation.pct_change- Compute change percentages.unique_counts- Count unique values in expressions.log- Logarithms for series.list_to_struct- ConvertListtoStructdata types.list_count- Count elements in lists.list_eval- Apply expressions over list elements.cumulative_eval- Apply expressions over cumulatively increasing windows.arg_where- Get indices where condition holds.search_sorted- Find indices where elements should be inserted to maintain order.offset_by- Add an offset to dates that take months and leap years into account.trigonometry- Trigonometric functions.sign- Compute the element-wise sign of a series.propagate_nans-NaN-propagating min/max aggregations.
is_in- Check for membership in Series.
is_in
zip_with- Zip twoSeries/ChunkedArrays.
zip_with
Series
ChunkedArray
round_series- round underlying float types of series.
round_series
repeat_by- Repeat element in an array a number of times specified by another array.
repeat_by
is_first_distinct- Check if element is first unique value.
is_first_distinct
is_last_distinct- Check if element is last unique value.
is_last_distinct
checked_arithmetic- checked arithmetic returningNoneon invalid operations.
checked_arithmetic
None
dot_product- Dot/inner product on series and expressions.
dot_product
concat_str- Concatenate string data in linear time.
concat_str
reinterpret- Utility to reinterpret bits to signed/unsigned.
reinterpret
take_opt_iter- Take from a series withIterator<Item=Option<usize>>.
take_opt_iter
Iterator<Item=Option<usize>>
mode- Return the most frequently occurring value(s).
mode
cum_agg-cum_sum,cum_min, andcum_max, aggregations.
cum_agg
cum_sum
cum_min
cum_max
rolling_window- rolling window functions, likerolling_mean.
rolling_window
rolling_mean
interpolate- Interpolate intermediateNonevalues.
interpolate
None
extract_jsonpath-Runjsonpathqueries onStringChunked.
extract_jsonpath
jsonpath
StringChunked
list- List utils:
list
list_gather- take sublist by multiple indices.
list_gather
rank- Ranking algorithms.
rank
moment- Kurtosis and skew statistics.
moment
ewma- Exponential moving average windows.
ewma
abs- Get absolute values of series.
abs
arange- Range operation on series.
arange
product- Compute the product of a series.
product
diff-diffoperation.
diff
diff
pct_change- Compute change percentages.
pct_change
unique_counts- Count unique values in expressions.
unique_counts
log- Logarithms for series.
log
list_to_struct- ConvertListtoStructdata types.
list_to_struct
List
Struct
list_count- Count elements in lists.
list_count
list_eval- Apply expressions over list elements.
list_eval
cumulative_eval- Apply expressions over cumulatively increasing windows.
cumulative_eval
arg_where- Get indices where condition holds.
arg_where
search_sorted- Find indices where elements should be inserted to maintain order.
search_sorted
offset_by- Add an offset to dates that take months and leap years into account.
offset_by
trigonometry- Trigonometric functions.
trigonometry
sign- Compute the element-wise sign of a series.
sign
propagate_nans-NaN-propagating min/max aggregations.
propagate_nans
NaN
Dataframe pretty printing:fmt- Activate dataframe formatting.
fmt- Activate dataframe formatting.
fmt
Only needed if you are on Windows.↩
Only needed if you are on Windows.↩



===== https://docs.pola.rs/user-guide/installation/#legacy-cpu =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting started
InstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
Installation
Polars is a library and installation is as simple as invoking the package manager of the
corresponding programming language.
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Big Index
By default, Polars dataframes are limited to\(2^{32}\)rows (~4.3 billion). Increase this limit to\(2^{64}\)(~18 quintillion) by enabling the big index extension:
pipinstallpolars-u64-idx
pipinstallpolars-u64-idx
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
Legacy CPU
To install Polars for Python on an old CPU withoutAVXsupport, run:
pipinstallpolars-lts-cpu
pipinstallpolars-lts-cpu
Importing
To use the library, simply import it into your project:
importpolarsaspl
importpolarsaspl
usepolars::prelude::*;
usepolars::prelude::*;
Feature flags
By using the above command you install the core of Polars onto your system. However, depending on
your use case, you might want to install the optional dependencies as well. These are made optional
to minimize the footprint. The flags are different depending on the programming language. Throughout
the user guide we will mention when a functionality used requires an additional dependency.
Python
# For example
pip install 'polars[numpy,fsspec]'
# For example
pip install 'polars[numpy,fsspec]'
Note
SeeGPU supportfor more detailed instructions and
prerequisites.
plot
style
Rust
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
The opt-in features are:
Additional data types:dtype-datedtype-datetimedtype-timedtype-durationdtype-i8dtype-i16dtype-u8dtype-u16dtype-categoricaldtype-struct
dtype-date
dtype-date
dtype-datetime
dtype-datetime
dtype-time
dtype-time
dtype-duration
dtype-duration
dtype-i8
dtype-i8
dtype-i16
dtype-i16
dtype-u8
dtype-u8
dtype-u16
dtype-u16
dtype-categorical
dtype-categorical
dtype-struct
dtype-struct
lazy- Lazy API:regex- Use regexes in column selection.dot_diagram- Create dot diagrams from lazy logical plans.
lazy
regex- Use regexes in column selection.
regex
dot_diagram- Create dot diagrams from lazy logical plans.
dot_diagram
sql- Pass SQL queries to Polars.
sql
streaming- Be able to process datasets that are larger than RAM.
streaming
random- Generate arrays with randomly sampled values
random
ndarray- Convert fromDataFrametondarray
ndarray
DataFrame
ndarray
temporal- Conversions betweenChronoand Polars for temporal data types
temporal
timezones- Activate timezone support.
timezones
strings- Extra string utilities forStringChunked:string_pad- forpad_start,pad_end,zfill.string_to_integer- forparse_int.
strings
StringChunked
string_pad- forpad_start,pad_end,zfill.
string_pad
pad_start
pad_end
zfill
string_to_integer- forparse_int.
string_to_integer
parse_int
object- Support for generic ChunkedArrays calledObjectChunked<T>(generic overT).
  These are downcastable from Series through theAnytrait.
object
ObjectChunked<T>
T
Performance related:nightly- Several nightly only features such as SIMD and specialization.performant- more fast paths, slower compile times.bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.cse- Activate common subplan elimination optimization.
nightly- Several nightly only features such as SIMD and specialization.
nightly
performant- more fast paths, slower compile times.
performant
bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.
bigidx
u64
cse- Activate common subplan elimination optimization.
cse
IO related:serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.parquet- Read Apache Parquet format.json- JSON serialization.ipc- Arrow's IPC format serialization.decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:gzipzlibzstd
serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde
serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde-lazy
parquet- Read Apache Parquet format.
parquet
json- JSON serialization.
json
ipc- Arrow's IPC format serialization.
ipc
decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:
decompress
gzip
zlib
zstd
Dataframe operations:dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.sort_multiple- Allow sorting a dataframe on multiple columns.rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.join_asof- Join ASOF, to join on nearest keys instead of exact equality match.cross_join- Create the Cartesian product of two dataframes.semi_anti_join- SEMI and ANTI joins.row_hash- Utility to hash dataframe rows toUInt64Chunked.diagonal_concat- Diagonal concatenation thereby combining different schemas.dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.partition_by- Split into multiple dataframes partitioned by groups.
dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.
dynamic_group_by
sort_multiple- Allow sorting a dataframe on multiple columns.
sort_multiple
rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.
rows
dataframes
pivot
transpose
join_asof- Join ASOF, to join on nearest keys instead of exact equality match.
join_asof
cross_join- Create the Cartesian product of two dataframes.
cross_join
semi_anti_join- SEMI and ANTI joins.
semi_anti_join
row_hash- Utility to hash dataframe rows toUInt64Chunked.
row_hash
UInt64Chunked
diagonal_concat- Diagonal concatenation thereby combining different schemas.
diagonal_concat
dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.
dataframe_arithmetic
partition_by- Split into multiple dataframes partitioned by groups.
partition_by
Series/expression operations:is_in- Check for membership in Series.zip_with- Zip twoSeries/ChunkedArrays.round_series- round underlying float types of series.repeat_by- Repeat element in an array a number of times specified by another array.is_first_distinct- Check if element is first unique value.is_last_distinct- Check if element is last unique value.checked_arithmetic- checked arithmetic returningNoneon invalid operations.dot_product- Dot/inner product on series and expressions.concat_str- Concatenate string data in linear time.reinterpret- Utility to reinterpret bits to signed/unsigned.take_opt_iter- Take from a series withIterator<Item=Option<usize>>.mode- Return the most frequently occurring value(s).cum_agg-cum_sum,cum_min, andcum_max, aggregations.rolling_window- rolling window functions, likerolling_mean.interpolate- Interpolate intermediateNonevalues.extract_jsonpath-Runjsonpathqueries onStringChunked.list- List utils:list_gather- take sublist by multiple indices.rank- Ranking algorithms.moment- Kurtosis and skew statistics.ewma- Exponential moving average windows.abs- Get absolute values of series.arange- Range operation on series.product- Compute the product of a series.diff-diffoperation.pct_change- Compute change percentages.unique_counts- Count unique values in expressions.log- Logarithms for series.list_to_struct- ConvertListtoStructdata types.list_count- Count elements in lists.list_eval- Apply expressions over list elements.cumulative_eval- Apply expressions over cumulatively increasing windows.arg_where- Get indices where condition holds.search_sorted- Find indices where elements should be inserted to maintain order.offset_by- Add an offset to dates that take months and leap years into account.trigonometry- Trigonometric functions.sign- Compute the element-wise sign of a series.propagate_nans-NaN-propagating min/max aggregations.
is_in- Check for membership in Series.
is_in
zip_with- Zip twoSeries/ChunkedArrays.
zip_with
Series
ChunkedArray
round_series- round underlying float types of series.
round_series
repeat_by- Repeat element in an array a number of times specified by another array.
repeat_by
is_first_distinct- Check if element is first unique value.
is_first_distinct
is_last_distinct- Check if element is last unique value.
is_last_distinct
checked_arithmetic- checked arithmetic returningNoneon invalid operations.
checked_arithmetic
None
dot_product- Dot/inner product on series and expressions.
dot_product
concat_str- Concatenate string data in linear time.
concat_str
reinterpret- Utility to reinterpret bits to signed/unsigned.
reinterpret
take_opt_iter- Take from a series withIterator<Item=Option<usize>>.
take_opt_iter
Iterator<Item=Option<usize>>
mode- Return the most frequently occurring value(s).
mode
cum_agg-cum_sum,cum_min, andcum_max, aggregations.
cum_agg
cum_sum
cum_min
cum_max
rolling_window- rolling window functions, likerolling_mean.
rolling_window
rolling_mean
interpolate- Interpolate intermediateNonevalues.
interpolate
None
extract_jsonpath-Runjsonpathqueries onStringChunked.
extract_jsonpath
jsonpath
StringChunked
list- List utils:
list
list_gather- take sublist by multiple indices.
list_gather
rank- Ranking algorithms.
rank
moment- Kurtosis and skew statistics.
moment
ewma- Exponential moving average windows.
ewma
abs- Get absolute values of series.
abs
arange- Range operation on series.
arange
product- Compute the product of a series.
product
diff-diffoperation.
diff
diff
pct_change- Compute change percentages.
pct_change
unique_counts- Count unique values in expressions.
unique_counts
log- Logarithms for series.
log
list_to_struct- ConvertListtoStructdata types.
list_to_struct
List
Struct
list_count- Count elements in lists.
list_count
list_eval- Apply expressions over list elements.
list_eval
cumulative_eval- Apply expressions over cumulatively increasing windows.
cumulative_eval
arg_where- Get indices where condition holds.
arg_where
search_sorted- Find indices where elements should be inserted to maintain order.
search_sorted
offset_by- Add an offset to dates that take months and leap years into account.
offset_by
trigonometry- Trigonometric functions.
trigonometry
sign- Compute the element-wise sign of a series.
sign
propagate_nans-NaN-propagating min/max aggregations.
propagate_nans
NaN
Dataframe pretty printing:fmt- Activate dataframe formatting.
fmt- Activate dataframe formatting.
fmt
Only needed if you are on Windows.↩
Only needed if you are on Windows.↩



===== https://docs.pola.rs/user-guide/installation/#importing =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting started
InstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
Installation
Polars is a library and installation is as simple as invoking the package manager of the
corresponding programming language.
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Big Index
By default, Polars dataframes are limited to\(2^{32}\)rows (~4.3 billion). Increase this limit to\(2^{64}\)(~18 quintillion) by enabling the big index extension:
pipinstallpolars-u64-idx
pipinstallpolars-u64-idx
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
Legacy CPU
To install Polars for Python on an old CPU withoutAVXsupport, run:
pipinstallpolars-lts-cpu
pipinstallpolars-lts-cpu
Importing
To use the library, simply import it into your project:
importpolarsaspl
importpolarsaspl
usepolars::prelude::*;
usepolars::prelude::*;
Feature flags
By using the above command you install the core of Polars onto your system. However, depending on
your use case, you might want to install the optional dependencies as well. These are made optional
to minimize the footprint. The flags are different depending on the programming language. Throughout
the user guide we will mention when a functionality used requires an additional dependency.
Python
# For example
pip install 'polars[numpy,fsspec]'
# For example
pip install 'polars[numpy,fsspec]'
Note
SeeGPU supportfor more detailed instructions and
prerequisites.
plot
style
Rust
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
The opt-in features are:
Additional data types:dtype-datedtype-datetimedtype-timedtype-durationdtype-i8dtype-i16dtype-u8dtype-u16dtype-categoricaldtype-struct
dtype-date
dtype-date
dtype-datetime
dtype-datetime
dtype-time
dtype-time
dtype-duration
dtype-duration
dtype-i8
dtype-i8
dtype-i16
dtype-i16
dtype-u8
dtype-u8
dtype-u16
dtype-u16
dtype-categorical
dtype-categorical
dtype-struct
dtype-struct
lazy- Lazy API:regex- Use regexes in column selection.dot_diagram- Create dot diagrams from lazy logical plans.
lazy
regex- Use regexes in column selection.
regex
dot_diagram- Create dot diagrams from lazy logical plans.
dot_diagram
sql- Pass SQL queries to Polars.
sql
streaming- Be able to process datasets that are larger than RAM.
streaming
random- Generate arrays with randomly sampled values
random
ndarray- Convert fromDataFrametondarray
ndarray
DataFrame
ndarray
temporal- Conversions betweenChronoand Polars for temporal data types
temporal
timezones- Activate timezone support.
timezones
strings- Extra string utilities forStringChunked:string_pad- forpad_start,pad_end,zfill.string_to_integer- forparse_int.
strings
StringChunked
string_pad- forpad_start,pad_end,zfill.
string_pad
pad_start
pad_end
zfill
string_to_integer- forparse_int.
string_to_integer
parse_int
object- Support for generic ChunkedArrays calledObjectChunked<T>(generic overT).
  These are downcastable from Series through theAnytrait.
object
ObjectChunked<T>
T
Performance related:nightly- Several nightly only features such as SIMD and specialization.performant- more fast paths, slower compile times.bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.cse- Activate common subplan elimination optimization.
nightly- Several nightly only features such as SIMD and specialization.
nightly
performant- more fast paths, slower compile times.
performant
bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.
bigidx
u64
cse- Activate common subplan elimination optimization.
cse
IO related:serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.parquet- Read Apache Parquet format.json- JSON serialization.ipc- Arrow's IPC format serialization.decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:gzipzlibzstd
serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde
serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde-lazy
parquet- Read Apache Parquet format.
parquet
json- JSON serialization.
json
ipc- Arrow's IPC format serialization.
ipc
decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:
decompress
gzip
zlib
zstd
Dataframe operations:dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.sort_multiple- Allow sorting a dataframe on multiple columns.rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.join_asof- Join ASOF, to join on nearest keys instead of exact equality match.cross_join- Create the Cartesian product of two dataframes.semi_anti_join- SEMI and ANTI joins.row_hash- Utility to hash dataframe rows toUInt64Chunked.diagonal_concat- Diagonal concatenation thereby combining different schemas.dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.partition_by- Split into multiple dataframes partitioned by groups.
dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.
dynamic_group_by
sort_multiple- Allow sorting a dataframe on multiple columns.
sort_multiple
rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.
rows
dataframes
pivot
transpose
join_asof- Join ASOF, to join on nearest keys instead of exact equality match.
join_asof
cross_join- Create the Cartesian product of two dataframes.
cross_join
semi_anti_join- SEMI and ANTI joins.
semi_anti_join
row_hash- Utility to hash dataframe rows toUInt64Chunked.
row_hash
UInt64Chunked
diagonal_concat- Diagonal concatenation thereby combining different schemas.
diagonal_concat
dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.
dataframe_arithmetic
partition_by- Split into multiple dataframes partitioned by groups.
partition_by
Series/expression operations:is_in- Check for membership in Series.zip_with- Zip twoSeries/ChunkedArrays.round_series- round underlying float types of series.repeat_by- Repeat element in an array a number of times specified by another array.is_first_distinct- Check if element is first unique value.is_last_distinct- Check if element is last unique value.checked_arithmetic- checked arithmetic returningNoneon invalid operations.dot_product- Dot/inner product on series and expressions.concat_str- Concatenate string data in linear time.reinterpret- Utility to reinterpret bits to signed/unsigned.take_opt_iter- Take from a series withIterator<Item=Option<usize>>.mode- Return the most frequently occurring value(s).cum_agg-cum_sum,cum_min, andcum_max, aggregations.rolling_window- rolling window functions, likerolling_mean.interpolate- Interpolate intermediateNonevalues.extract_jsonpath-Runjsonpathqueries onStringChunked.list- List utils:list_gather- take sublist by multiple indices.rank- Ranking algorithms.moment- Kurtosis and skew statistics.ewma- Exponential moving average windows.abs- Get absolute values of series.arange- Range operation on series.product- Compute the product of a series.diff-diffoperation.pct_change- Compute change percentages.unique_counts- Count unique values in expressions.log- Logarithms for series.list_to_struct- ConvertListtoStructdata types.list_count- Count elements in lists.list_eval- Apply expressions over list elements.cumulative_eval- Apply expressions over cumulatively increasing windows.arg_where- Get indices where condition holds.search_sorted- Find indices where elements should be inserted to maintain order.offset_by- Add an offset to dates that take months and leap years into account.trigonometry- Trigonometric functions.sign- Compute the element-wise sign of a series.propagate_nans-NaN-propagating min/max aggregations.
is_in- Check for membership in Series.
is_in
zip_with- Zip twoSeries/ChunkedArrays.
zip_with
Series
ChunkedArray
round_series- round underlying float types of series.
round_series
repeat_by- Repeat element in an array a number of times specified by another array.
repeat_by
is_first_distinct- Check if element is first unique value.
is_first_distinct
is_last_distinct- Check if element is last unique value.
is_last_distinct
checked_arithmetic- checked arithmetic returningNoneon invalid operations.
checked_arithmetic
None
dot_product- Dot/inner product on series and expressions.
dot_product
concat_str- Concatenate string data in linear time.
concat_str
reinterpret- Utility to reinterpret bits to signed/unsigned.
reinterpret
take_opt_iter- Take from a series withIterator<Item=Option<usize>>.
take_opt_iter
Iterator<Item=Option<usize>>
mode- Return the most frequently occurring value(s).
mode
cum_agg-cum_sum,cum_min, andcum_max, aggregations.
cum_agg
cum_sum
cum_min
cum_max
rolling_window- rolling window functions, likerolling_mean.
rolling_window
rolling_mean
interpolate- Interpolate intermediateNonevalues.
interpolate
None
extract_jsonpath-Runjsonpathqueries onStringChunked.
extract_jsonpath
jsonpath
StringChunked
list- List utils:
list
list_gather- take sublist by multiple indices.
list_gather
rank- Ranking algorithms.
rank
moment- Kurtosis and skew statistics.
moment
ewma- Exponential moving average windows.
ewma
abs- Get absolute values of series.
abs
arange- Range operation on series.
arange
product- Compute the product of a series.
product
diff-diffoperation.
diff
diff
pct_change- Compute change percentages.
pct_change
unique_counts- Count unique values in expressions.
unique_counts
log- Logarithms for series.
log
list_to_struct- ConvertListtoStructdata types.
list_to_struct
List
Struct
list_count- Count elements in lists.
list_count
list_eval- Apply expressions over list elements.
list_eval
cumulative_eval- Apply expressions over cumulatively increasing windows.
cumulative_eval
arg_where- Get indices where condition holds.
arg_where
search_sorted- Find indices where elements should be inserted to maintain order.
search_sorted
offset_by- Add an offset to dates that take months and leap years into account.
offset_by
trigonometry- Trigonometric functions.
trigonometry
sign- Compute the element-wise sign of a series.
sign
propagate_nans-NaN-propagating min/max aggregations.
propagate_nans
NaN
Dataframe pretty printing:fmt- Activate dataframe formatting.
fmt- Activate dataframe formatting.
fmt
Only needed if you are on Windows.↩
Only needed if you are on Windows.↩



===== https://docs.pola.rs/user-guide/installation/#feature-flags =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting started
InstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
Installation
Polars is a library and installation is as simple as invoking the package manager of the
corresponding programming language.
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Big Index
By default, Polars dataframes are limited to\(2^{32}\)rows (~4.3 billion). Increase this limit to\(2^{64}\)(~18 quintillion) by enabling the big index extension:
pipinstallpolars-u64-idx
pipinstallpolars-u64-idx
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
Legacy CPU
To install Polars for Python on an old CPU withoutAVXsupport, run:
pipinstallpolars-lts-cpu
pipinstallpolars-lts-cpu
Importing
To use the library, simply import it into your project:
importpolarsaspl
importpolarsaspl
usepolars::prelude::*;
usepolars::prelude::*;
Feature flags
By using the above command you install the core of Polars onto your system. However, depending on
your use case, you might want to install the optional dependencies as well. These are made optional
to minimize the footprint. The flags are different depending on the programming language. Throughout
the user guide we will mention when a functionality used requires an additional dependency.
Python
# For example
pip install 'polars[numpy,fsspec]'
# For example
pip install 'polars[numpy,fsspec]'
Note
SeeGPU supportfor more detailed instructions and
prerequisites.
plot
style
Rust
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
The opt-in features are:
Additional data types:dtype-datedtype-datetimedtype-timedtype-durationdtype-i8dtype-i16dtype-u8dtype-u16dtype-categoricaldtype-struct
dtype-date
dtype-date
dtype-datetime
dtype-datetime
dtype-time
dtype-time
dtype-duration
dtype-duration
dtype-i8
dtype-i8
dtype-i16
dtype-i16
dtype-u8
dtype-u8
dtype-u16
dtype-u16
dtype-categorical
dtype-categorical
dtype-struct
dtype-struct
lazy- Lazy API:regex- Use regexes in column selection.dot_diagram- Create dot diagrams from lazy logical plans.
lazy
regex- Use regexes in column selection.
regex
dot_diagram- Create dot diagrams from lazy logical plans.
dot_diagram
sql- Pass SQL queries to Polars.
sql
streaming- Be able to process datasets that are larger than RAM.
streaming
random- Generate arrays with randomly sampled values
random
ndarray- Convert fromDataFrametondarray
ndarray
DataFrame
ndarray
temporal- Conversions betweenChronoand Polars for temporal data types
temporal
timezones- Activate timezone support.
timezones
strings- Extra string utilities forStringChunked:string_pad- forpad_start,pad_end,zfill.string_to_integer- forparse_int.
strings
StringChunked
string_pad- forpad_start,pad_end,zfill.
string_pad
pad_start
pad_end
zfill
string_to_integer- forparse_int.
string_to_integer
parse_int
object- Support for generic ChunkedArrays calledObjectChunked<T>(generic overT).
  These are downcastable from Series through theAnytrait.
object
ObjectChunked<T>
T
Performance related:nightly- Several nightly only features such as SIMD and specialization.performant- more fast paths, slower compile times.bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.cse- Activate common subplan elimination optimization.
nightly- Several nightly only features such as SIMD and specialization.
nightly
performant- more fast paths, slower compile times.
performant
bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.
bigidx
u64
cse- Activate common subplan elimination optimization.
cse
IO related:serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.parquet- Read Apache Parquet format.json- JSON serialization.ipc- Arrow's IPC format serialization.decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:gzipzlibzstd
serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde
serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde-lazy
parquet- Read Apache Parquet format.
parquet
json- JSON serialization.
json
ipc- Arrow's IPC format serialization.
ipc
decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:
decompress
gzip
zlib
zstd
Dataframe operations:dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.sort_multiple- Allow sorting a dataframe on multiple columns.rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.join_asof- Join ASOF, to join on nearest keys instead of exact equality match.cross_join- Create the Cartesian product of two dataframes.semi_anti_join- SEMI and ANTI joins.row_hash- Utility to hash dataframe rows toUInt64Chunked.diagonal_concat- Diagonal concatenation thereby combining different schemas.dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.partition_by- Split into multiple dataframes partitioned by groups.
dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.
dynamic_group_by
sort_multiple- Allow sorting a dataframe on multiple columns.
sort_multiple
rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.
rows
dataframes
pivot
transpose
join_asof- Join ASOF, to join on nearest keys instead of exact equality match.
join_asof
cross_join- Create the Cartesian product of two dataframes.
cross_join
semi_anti_join- SEMI and ANTI joins.
semi_anti_join
row_hash- Utility to hash dataframe rows toUInt64Chunked.
row_hash
UInt64Chunked
diagonal_concat- Diagonal concatenation thereby combining different schemas.
diagonal_concat
dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.
dataframe_arithmetic
partition_by- Split into multiple dataframes partitioned by groups.
partition_by
Series/expression operations:is_in- Check for membership in Series.zip_with- Zip twoSeries/ChunkedArrays.round_series- round underlying float types of series.repeat_by- Repeat element in an array a number of times specified by another array.is_first_distinct- Check if element is first unique value.is_last_distinct- Check if element is last unique value.checked_arithmetic- checked arithmetic returningNoneon invalid operations.dot_product- Dot/inner product on series and expressions.concat_str- Concatenate string data in linear time.reinterpret- Utility to reinterpret bits to signed/unsigned.take_opt_iter- Take from a series withIterator<Item=Option<usize>>.mode- Return the most frequently occurring value(s).cum_agg-cum_sum,cum_min, andcum_max, aggregations.rolling_window- rolling window functions, likerolling_mean.interpolate- Interpolate intermediateNonevalues.extract_jsonpath-Runjsonpathqueries onStringChunked.list- List utils:list_gather- take sublist by multiple indices.rank- Ranking algorithms.moment- Kurtosis and skew statistics.ewma- Exponential moving average windows.abs- Get absolute values of series.arange- Range operation on series.product- Compute the product of a series.diff-diffoperation.pct_change- Compute change percentages.unique_counts- Count unique values in expressions.log- Logarithms for series.list_to_struct- ConvertListtoStructdata types.list_count- Count elements in lists.list_eval- Apply expressions over list elements.cumulative_eval- Apply expressions over cumulatively increasing windows.arg_where- Get indices where condition holds.search_sorted- Find indices where elements should be inserted to maintain order.offset_by- Add an offset to dates that take months and leap years into account.trigonometry- Trigonometric functions.sign- Compute the element-wise sign of a series.propagate_nans-NaN-propagating min/max aggregations.
is_in- Check for membership in Series.
is_in
zip_with- Zip twoSeries/ChunkedArrays.
zip_with
Series
ChunkedArray
round_series- round underlying float types of series.
round_series
repeat_by- Repeat element in an array a number of times specified by another array.
repeat_by
is_first_distinct- Check if element is first unique value.
is_first_distinct
is_last_distinct- Check if element is last unique value.
is_last_distinct
checked_arithmetic- checked arithmetic returningNoneon invalid operations.
checked_arithmetic
None
dot_product- Dot/inner product on series and expressions.
dot_product
concat_str- Concatenate string data in linear time.
concat_str
reinterpret- Utility to reinterpret bits to signed/unsigned.
reinterpret
take_opt_iter- Take from a series withIterator<Item=Option<usize>>.
take_opt_iter
Iterator<Item=Option<usize>>
mode- Return the most frequently occurring value(s).
mode
cum_agg-cum_sum,cum_min, andcum_max, aggregations.
cum_agg
cum_sum
cum_min
cum_max
rolling_window- rolling window functions, likerolling_mean.
rolling_window
rolling_mean
interpolate- Interpolate intermediateNonevalues.
interpolate
None
extract_jsonpath-Runjsonpathqueries onStringChunked.
extract_jsonpath
jsonpath
StringChunked
list- List utils:
list
list_gather- take sublist by multiple indices.
list_gather
rank- Ranking algorithms.
rank
moment- Kurtosis and skew statistics.
moment
ewma- Exponential moving average windows.
ewma
abs- Get absolute values of series.
abs
arange- Range operation on series.
arange
product- Compute the product of a series.
product
diff-diffoperation.
diff
diff
pct_change- Compute change percentages.
pct_change
unique_counts- Count unique values in expressions.
unique_counts
log- Logarithms for series.
log
list_to_struct- ConvertListtoStructdata types.
list_to_struct
List
Struct
list_count- Count elements in lists.
list_count
list_eval- Apply expressions over list elements.
list_eval
cumulative_eval- Apply expressions over cumulatively increasing windows.
cumulative_eval
arg_where- Get indices where condition holds.
arg_where
search_sorted- Find indices where elements should be inserted to maintain order.
search_sorted
offset_by- Add an offset to dates that take months and leap years into account.
offset_by
trigonometry- Trigonometric functions.
trigonometry
sign- Compute the element-wise sign of a series.
sign
propagate_nans-NaN-propagating min/max aggregations.
propagate_nans
NaN
Dataframe pretty printing:fmt- Activate dataframe formatting.
fmt- Activate dataframe formatting.
fmt
Only needed if you are on Windows.↩
Only needed if you are on Windows.↩



===== https://docs.pola.rs/user-guide/installation/#python =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting started
InstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
Installation
Polars is a library and installation is as simple as invoking the package manager of the
corresponding programming language.
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Big Index
By default, Polars dataframes are limited to\(2^{32}\)rows (~4.3 billion). Increase this limit to\(2^{64}\)(~18 quintillion) by enabling the big index extension:
pipinstallpolars-u64-idx
pipinstallpolars-u64-idx
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
Legacy CPU
To install Polars for Python on an old CPU withoutAVXsupport, run:
pipinstallpolars-lts-cpu
pipinstallpolars-lts-cpu
Importing
To use the library, simply import it into your project:
importpolarsaspl
importpolarsaspl
usepolars::prelude::*;
usepolars::prelude::*;
Feature flags
By using the above command you install the core of Polars onto your system. However, depending on
your use case, you might want to install the optional dependencies as well. These are made optional
to minimize the footprint. The flags are different depending on the programming language. Throughout
the user guide we will mention when a functionality used requires an additional dependency.
Python
# For example
pip install 'polars[numpy,fsspec]'
# For example
pip install 'polars[numpy,fsspec]'
Note
SeeGPU supportfor more detailed instructions and
prerequisites.
plot
style
Rust
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
The opt-in features are:
Additional data types:dtype-datedtype-datetimedtype-timedtype-durationdtype-i8dtype-i16dtype-u8dtype-u16dtype-categoricaldtype-struct
dtype-date
dtype-date
dtype-datetime
dtype-datetime
dtype-time
dtype-time
dtype-duration
dtype-duration
dtype-i8
dtype-i8
dtype-i16
dtype-i16
dtype-u8
dtype-u8
dtype-u16
dtype-u16
dtype-categorical
dtype-categorical
dtype-struct
dtype-struct
lazy- Lazy API:regex- Use regexes in column selection.dot_diagram- Create dot diagrams from lazy logical plans.
lazy
regex- Use regexes in column selection.
regex
dot_diagram- Create dot diagrams from lazy logical plans.
dot_diagram
sql- Pass SQL queries to Polars.
sql
streaming- Be able to process datasets that are larger than RAM.
streaming
random- Generate arrays with randomly sampled values
random
ndarray- Convert fromDataFrametondarray
ndarray
DataFrame
ndarray
temporal- Conversions betweenChronoand Polars for temporal data types
temporal
timezones- Activate timezone support.
timezones
strings- Extra string utilities forStringChunked:string_pad- forpad_start,pad_end,zfill.string_to_integer- forparse_int.
strings
StringChunked
string_pad- forpad_start,pad_end,zfill.
string_pad
pad_start
pad_end
zfill
string_to_integer- forparse_int.
string_to_integer
parse_int
object- Support for generic ChunkedArrays calledObjectChunked<T>(generic overT).
  These are downcastable from Series through theAnytrait.
object
ObjectChunked<T>
T
Performance related:nightly- Several nightly only features such as SIMD and specialization.performant- more fast paths, slower compile times.bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.cse- Activate common subplan elimination optimization.
nightly- Several nightly only features such as SIMD and specialization.
nightly
performant- more fast paths, slower compile times.
performant
bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.
bigidx
u64
cse- Activate common subplan elimination optimization.
cse
IO related:serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.parquet- Read Apache Parquet format.json- JSON serialization.ipc- Arrow's IPC format serialization.decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:gzipzlibzstd
serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde
serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde-lazy
parquet- Read Apache Parquet format.
parquet
json- JSON serialization.
json
ipc- Arrow's IPC format serialization.
ipc
decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:
decompress
gzip
zlib
zstd
Dataframe operations:dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.sort_multiple- Allow sorting a dataframe on multiple columns.rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.join_asof- Join ASOF, to join on nearest keys instead of exact equality match.cross_join- Create the Cartesian product of two dataframes.semi_anti_join- SEMI and ANTI joins.row_hash- Utility to hash dataframe rows toUInt64Chunked.diagonal_concat- Diagonal concatenation thereby combining different schemas.dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.partition_by- Split into multiple dataframes partitioned by groups.
dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.
dynamic_group_by
sort_multiple- Allow sorting a dataframe on multiple columns.
sort_multiple
rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.
rows
dataframes
pivot
transpose
join_asof- Join ASOF, to join on nearest keys instead of exact equality match.
join_asof
cross_join- Create the Cartesian product of two dataframes.
cross_join
semi_anti_join- SEMI and ANTI joins.
semi_anti_join
row_hash- Utility to hash dataframe rows toUInt64Chunked.
row_hash
UInt64Chunked
diagonal_concat- Diagonal concatenation thereby combining different schemas.
diagonal_concat
dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.
dataframe_arithmetic
partition_by- Split into multiple dataframes partitioned by groups.
partition_by
Series/expression operations:is_in- Check for membership in Series.zip_with- Zip twoSeries/ChunkedArrays.round_series- round underlying float types of series.repeat_by- Repeat element in an array a number of times specified by another array.is_first_distinct- Check if element is first unique value.is_last_distinct- Check if element is last unique value.checked_arithmetic- checked arithmetic returningNoneon invalid operations.dot_product- Dot/inner product on series and expressions.concat_str- Concatenate string data in linear time.reinterpret- Utility to reinterpret bits to signed/unsigned.take_opt_iter- Take from a series withIterator<Item=Option<usize>>.mode- Return the most frequently occurring value(s).cum_agg-cum_sum,cum_min, andcum_max, aggregations.rolling_window- rolling window functions, likerolling_mean.interpolate- Interpolate intermediateNonevalues.extract_jsonpath-Runjsonpathqueries onStringChunked.list- List utils:list_gather- take sublist by multiple indices.rank- Ranking algorithms.moment- Kurtosis and skew statistics.ewma- Exponential moving average windows.abs- Get absolute values of series.arange- Range operation on series.product- Compute the product of a series.diff-diffoperation.pct_change- Compute change percentages.unique_counts- Count unique values in expressions.log- Logarithms for series.list_to_struct- ConvertListtoStructdata types.list_count- Count elements in lists.list_eval- Apply expressions over list elements.cumulative_eval- Apply expressions over cumulatively increasing windows.arg_where- Get indices where condition holds.search_sorted- Find indices where elements should be inserted to maintain order.offset_by- Add an offset to dates that take months and leap years into account.trigonometry- Trigonometric functions.sign- Compute the element-wise sign of a series.propagate_nans-NaN-propagating min/max aggregations.
is_in- Check for membership in Series.
is_in
zip_with- Zip twoSeries/ChunkedArrays.
zip_with
Series
ChunkedArray
round_series- round underlying float types of series.
round_series
repeat_by- Repeat element in an array a number of times specified by another array.
repeat_by
is_first_distinct- Check if element is first unique value.
is_first_distinct
is_last_distinct- Check if element is last unique value.
is_last_distinct
checked_arithmetic- checked arithmetic returningNoneon invalid operations.
checked_arithmetic
None
dot_product- Dot/inner product on series and expressions.
dot_product
concat_str- Concatenate string data in linear time.
concat_str
reinterpret- Utility to reinterpret bits to signed/unsigned.
reinterpret
take_opt_iter- Take from a series withIterator<Item=Option<usize>>.
take_opt_iter
Iterator<Item=Option<usize>>
mode- Return the most frequently occurring value(s).
mode
cum_agg-cum_sum,cum_min, andcum_max, aggregations.
cum_agg
cum_sum
cum_min
cum_max
rolling_window- rolling window functions, likerolling_mean.
rolling_window
rolling_mean
interpolate- Interpolate intermediateNonevalues.
interpolate
None
extract_jsonpath-Runjsonpathqueries onStringChunked.
extract_jsonpath
jsonpath
StringChunked
list- List utils:
list
list_gather- take sublist by multiple indices.
list_gather
rank- Ranking algorithms.
rank
moment- Kurtosis and skew statistics.
moment
ewma- Exponential moving average windows.
ewma
abs- Get absolute values of series.
abs
arange- Range operation on series.
arange
product- Compute the product of a series.
product
diff-diffoperation.
diff
diff
pct_change- Compute change percentages.
pct_change
unique_counts- Count unique values in expressions.
unique_counts
log- Logarithms for series.
log
list_to_struct- ConvertListtoStructdata types.
list_to_struct
List
Struct
list_count- Count elements in lists.
list_count
list_eval- Apply expressions over list elements.
list_eval
cumulative_eval- Apply expressions over cumulatively increasing windows.
cumulative_eval
arg_where- Get indices where condition holds.
arg_where
search_sorted- Find indices where elements should be inserted to maintain order.
search_sorted
offset_by- Add an offset to dates that take months and leap years into account.
offset_by
trigonometry- Trigonometric functions.
trigonometry
sign- Compute the element-wise sign of a series.
sign
propagate_nans-NaN-propagating min/max aggregations.
propagate_nans
NaN
Dataframe pretty printing:fmt- Activate dataframe formatting.
fmt- Activate dataframe formatting.
fmt
Only needed if you are on Windows.↩
Only needed if you are on Windows.↩



===== https://docs.pola.rs/user-guide/installation/#all =====

Polars
Polars Cloud
PolarsPolarsUser guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]APIAPIReference guideDevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioningReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
User guideUser guideGetting startedInstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRustConceptsConceptsData types and structuresExpressions and contextsLazy APIStreamingExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functionsTransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zonesLazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU SupportIOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)PluginsPluginsExpression PluginsIO PluginsSQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table ExpressionsMigratingMigratingComing from PandasComing from Apache SparkMiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMsGPU Support [Open Beta]
Getting started
InstallationInstallationTable of contentsBig IndexLegacy CPUImportingFeature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
ConceptsConceptsData types and structuresExpressions and contextsLazy APIStreaming
Data types and structures
Expressions and contexts
Lazy API
Streaming
ExpressionsExpressionsBasic operationsExpression expansionCastingStringsLists and arraysCategorical data and enumsStructsMissing dataAggregationWindow functionsFoldsUser-defined Python functionsNumpy functions
Basic operations
Expression expansion
Casting
Strings
Lists and arrays
Categorical data and enums
Structs
Missing data
Aggregation
Window functions
Folds
User-defined Python functions
Numpy functions
TransformationsTransformationsJoinsConcatenationPivotsUnpivotsTime seriesTime seriesParsingFilteringGroupingResamplingTime zones
Joins
Concatenation
Pivots
Unpivots
Time seriesTime seriesParsingFilteringGroupingResamplingTime zones
Parsing
Filtering
Grouping
Resampling
Time zones
Lazy APILazy APIUsageOptimizationsSchemaDataType ExpressionsQuery planQuery executionSources and sinksMultiplexing queriesGPU Support
Usage
Optimizations
Schema
DataType Expressions
Query plan
Query execution
Sources and sinks
Multiplexing queries
GPU Support
IOIOCSVExcelParquetJSON filesMultipleHiveDatabasesCloud storageGoogle BigQueryHugging FaceGoogle Sheets (via Colab)
CSV
Excel
Parquet
JSON files
Multiple
Hive
Databases
Cloud storage
Google BigQuery
Hugging Face
Google Sheets (via Colab)
PluginsPluginsExpression PluginsIO Plugins
Expression Plugins
IO Plugins
SQLSQLIntroductionSHOW TABLESSELECTCREATECommon Table Expressions
Introduction
SHOW TABLES
SELECT
CREATE
Common Table Expressions
MigratingMigratingComing from PandasComing from Apache Spark
Coming from Pandas
Coming from Apache Spark
MiscMiscEcosystemMultiprocessingVisualizationStylingComparison with other toolsArrow producer/consumerGenerating Polars code with LLMs
Ecosystem
Multiprocessing
Visualization
Styling
Comparison with other tools
Arrow producer/consumer
Generating Polars code with LLMs
GPU Support [Open Beta]
APIAPIReference guide
Reference guide
DevelopmentDevelopmentContributingContributingIDE configurationTest suiteContinuous integrationCode styleVersioning
ContributingContributingIDE configurationTest suiteContinuous integrationCode style
IDE configuration
Test suite
Continuous integration
Code style
Versioning
ReleasesReleasesUpgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19Changelog
Upgrade guidesUpgrade guidesVersion 1Version 0.20Version 0.19
Version 1
Version 0.20
Version 0.19
Changelog
Polars CloudPolars CloudGetting startedConnect to your cloudQueriesQueriesExecute remote queryDistributed queriesQuery progress monitoringIntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS LambdaConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease NotesAPIAPIReference guide
Getting started
Connect to your cloud
QueriesQueriesExecute remote queryDistributed queriesQuery progress monitoring
Execute remote query
Distributed queries
Query progress monitoring
IntegrationsIntegrationsOrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
OrchestrationOrchestrationAirflowDagsterPrefectAWS Lambda
Airflow
Dagster
Prefect
AWS Lambda
ConceptsConceptsContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy modeOrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage membersWorkspacesWorkspacesWorkspace configurationManage teamAuthenticationAuthenticationLogging inUsing service accountsProvidersProvidersAWSAWSInfrastructurePermissionsMiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
ContextContextCompute context introductionReconnect to compute clusterPlugins and custom librariesProxy mode
Compute context introduction
Reconnect to compute cluster
Plugins and custom libraries
Proxy mode
OrganizationsOrganizationsSet up organizationStart trial periodPayment and billingManage members
Set up organization
Start trial period
Payment and billing
Manage members
WorkspacesWorkspacesWorkspace configurationManage team
Workspace configuration
Manage team
AuthenticationAuthenticationLogging inUsing service accounts
Logging in
Using service accounts
ProvidersProvidersAWSAWSInfrastructurePermissions
AWSAWSInfrastructurePermissions
Infrastructure
Permissions
MiscMiscCLIPublic datasetsFAQAPI ReferenceRelease Notes
CLI
Public datasets
FAQ
API Reference
Release Notes
APIAPIReference guide
Reference guide
Big Index
Legacy CPU
Importing
Feature flagsPythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOtherRust
PythonAllGPUInteroperabilityExcelDatabaseCloudOther I/OOther
All
GPU
Interoperability
Excel
Database
Cloud
Other I/O
Other
Rust
Installation
Polars is a library and installation is as simple as invoking the package manager of the
corresponding programming language.
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
pipinstallpolars# Or for legacy CPUs without AVX2 supportpipinstallpolars-lts-cpu
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
cargoaddpolars-Flazy# Or Cargo.toml[dependencies]polars={version="x",features=["lazy",...]}
Big Index
By default, Polars dataframes are limited to\(2^{32}\)rows (~4.3 billion). Increase this limit to\(2^{64}\)(~18 quintillion) by enabling the big index extension:
pipinstallpolars-u64-idx
pipinstallpolars-u64-idx
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
cargoaddpolars-Fbigidx# Or Cargo.toml[dependencies]polars={version="x",features=["bigidx",...]}
Legacy CPU
To install Polars for Python on an old CPU withoutAVXsupport, run:
pipinstallpolars-lts-cpu
pipinstallpolars-lts-cpu
Importing
To use the library, simply import it into your project:
importpolarsaspl
importpolarsaspl
usepolars::prelude::*;
usepolars::prelude::*;
Feature flags
By using the above command you install the core of Polars onto your system. However, depending on
your use case, you might want to install the optional dependencies as well. These are made optional
to minimize the footprint. The flags are different depending on the programming language. Throughout
the user guide we will mention when a functionality used requires an additional dependency.
Python
# For example
pip install 'polars[numpy,fsspec]'
# For example
pip install 'polars[numpy,fsspec]'
Note
SeeGPU supportfor more detailed instructions and
prerequisites.
plot
style
Rust
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
# Cargo.toml[dependencies]polars={version="0.26.1",features=["lazy","temporal","describe","json","parquet","dtype-datetime"]}
The opt-in features are:
Additional data types:dtype-datedtype-datetimedtype-timedtype-durationdtype-i8dtype-i16dtype-u8dtype-u16dtype-categoricaldtype-struct
dtype-date
dtype-date
dtype-datetime
dtype-datetime
dtype-time
dtype-time
dtype-duration
dtype-duration
dtype-i8
dtype-i8
dtype-i16
dtype-i16
dtype-u8
dtype-u8
dtype-u16
dtype-u16
dtype-categorical
dtype-categorical
dtype-struct
dtype-struct
lazy- Lazy API:regex- Use regexes in column selection.dot_diagram- Create dot diagrams from lazy logical plans.
lazy
regex- Use regexes in column selection.
regex
dot_diagram- Create dot diagrams from lazy logical plans.
dot_diagram
sql- Pass SQL queries to Polars.
sql
streaming- Be able to process datasets that are larger than RAM.
streaming
random- Generate arrays with randomly sampled values
random
ndarray- Convert fromDataFrametondarray
ndarray
DataFrame
ndarray
temporal- Conversions betweenChronoand Polars for temporal data types
temporal
timezones- Activate timezone support.
timezones
strings- Extra string utilities forStringChunked:string_pad- forpad_start,pad_end,zfill.string_to_integer- forparse_int.
strings
StringChunked
string_pad- forpad_start,pad_end,zfill.
string_pad
pad_start
pad_end
zfill
string_to_integer- forparse_int.
string_to_integer
parse_int
object- Support for generic ChunkedArrays calledObjectChunked<T>(generic overT).
  These are downcastable from Series through theAnytrait.
object
ObjectChunked<T>
T
Performance related:nightly- Several nightly only features such as SIMD and specialization.performant- more fast paths, slower compile times.bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.cse- Activate common subplan elimination optimization.
nightly- Several nightly only features such as SIMD and specialization.
nightly
performant- more fast paths, slower compile times.
performant
bigidx- Activate this feature if you expect >>\(2^{32}\)rows.
This allows polars to scale up way beyond that by usingu64as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.
bigidx
u64
cse- Activate common subplan elimination optimization.
cse
IO related:serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.parquet- Read Apache Parquet format.json- JSON serialization.ipc- Arrow's IPC format serialization.decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:gzipzlibzstd
serde- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde
serde-lazy- Support forserdeserialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
serde-lazy
parquet- Read Apache Parquet format.
parquet
json- JSON serialization.
json
ipc- Arrow's IPC format serialization.
ipc
decompress- Automatically infer compression of csvs and decompress them.
Supported compressions:
decompress
gzip
zlib
zstd
Dataframe operations:dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.sort_multiple- Allow sorting a dataframe on multiple columns.rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.join_asof- Join ASOF, to join on nearest keys instead of exact equality match.cross_join- Create the Cartesian product of two dataframes.semi_anti_join- SEMI and ANTI joins.row_hash- Utility to hash dataframe rows toUInt64Chunked.diagonal_concat- Diagonal concatenation thereby combining different schemas.dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.partition_by- Split into multiple dataframes partitioned by groups.
dynamic_group_by- Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.
dynamic_group_by
sort_multiple- Allow sorting a dataframe on multiple columns.
sort_multiple
rows- Create dataframe from rows and extract rows fromdataframes.
Also activatespivotandtransposeoperations.
rows
dataframes
pivot
transpose
join_asof- Join ASOF, to join on nearest keys instead of exact equality match.
join_asof
cross_join- Create the Cartesian product of two dataframes.
cross_join
semi_anti_join- SEMI and ANTI joins.
semi_anti_join
row_hash- Utility to hash dataframe rows toUInt64Chunked.
row_hash
UInt64Chunked
diagonal_concat- Diagonal concatenation thereby combining different schemas.
diagonal_concat
dataframe_arithmetic- Arithmetic between dataframes and other dataframes or series.
dataframe_arithmetic
partition_by- Split into multiple dataframes partitioned by groups.
partition_by
Series/expression operations:is_in- Check for membership in Series.zip_with- Zip twoSeries/ChunkedArrays.round_series- round underlying float types of series.repeat_by- Repeat element in an array a number of times specified by another array.is_first_distinct- Check if element is first unique value.is_last_distinct- Check if element is last unique value.checked_arithmetic- checked arithmetic returningNoneon invalid operations.dot_product- Dot/inner product on series and expressions.concat_str- Concatenate string data in linear time.reinterpret- Utility to reinterpret bits to signed/unsigned.take_opt_iter- Take from a series withIterator<Item=Option<usize>>.mode- Return the most frequently occurring value(s).cum_agg-cum_sum,cum_min, andcum_max, aggregations.rolling_window- rolling window functions, likerolling_mean.interpolate- Interpolate intermediateNonevalues.extract_jsonpath-Runjsonpathqueries onStringChunked.list- List utils:list_gather- take sublist by multiple indices.rank- Ranking algorithms.moment- Kurtosis and skew statistics.ewma- Exponential moving average windows.abs- Get absolute values of series.arange- Range operation on series.product- Compute the product of a series.diff-diffoperation.pct_change- Compute change percentages.unique_counts- Count unique values in expressions.log- Logarithms for series.list_to_struct- ConvertListtoStructdata types.list_count- Count elements in lists.list_eval- Apply expressions over list elements.cumulative_eval- Apply expressions over cumulatively increasing windows.arg_where- Get indices where condition holds.search_sorted- Find indices where elements should be inserted to maintain order.offset_by- Add an offset to dates that take months and leap years into account.trigonometry- Trigonometric functions.sign- Compute the element-wise sign of a series.propagate_nans-NaN-propagating min/max aggregations.
is_in- Check for membership in Series.
is_in
zip_with- Zip twoSeries/ChunkedArrays.
zip_with
Series
ChunkedArray
round_series- round underlying float types of series.
round_series
repeat_by- Repeat element in an array a number of times specified by another array.
repeat_by
is_first_distinct- Check if element is first unique value.
is_first_distinct
is_last_distinct- Check if element is last unique value.
is_last_distinct
checked_arithmetic- checked arithmetic returningNoneon invalid operations.
checked_arithmetic
None
dot_product- Dot/inner product on series and expressions.
dot_product
concat_str- Concatenate string data in linear time.
concat_str
reinterpret- Utility to reinterpret bits to signed/unsigned.
reinterpret
take_opt_iter- Take from a series withIterator<Item=Option<usize>>.
take_opt_iter
Iterator<Item=Option<usize>>
mode- Return the most frequently occurring value(s).
mode
cum_agg-cum_sum,cum_min, andcum_max, aggregations.
cum_agg
cum_sum
cum_min
cum_max
rolling_window- rolling window functions, likerolling_mean.
rolling_window
rolling_mean
interpolate- Interpolate intermediateNonevalues.
interpolate
None
extract_jsonpath-Runjsonpathqueries onStringChunked.
extract_jsonpath
jsonpath
StringChunked
list- List utils:
list
list_gather- take sublist by multiple indices.
list_gather
rank- Ranking algorithms.
rank
moment- Kurtosis and skew statistics.
moment
ewma- Exponential moving average windows.
ewma
abs- Get absolute values of series.
abs
arange- Range operation on series.
arange
product- Compute the product of a series.
product
diff-diffoperation.
diff
diff
pct_change- Compute change percentages.
pct_change
unique_counts- Count unique values in expressions.
unique_counts
log- Logarithms for series.
log
list_to_struct- ConvertListtoStructdata types.
list_to_struct
List
Struct
list_count- Count elements in lists.
list_count
list_eval- Apply expressions over list elements.
list_eval
cumulative_eval- Apply expressions over cumulatively increasing windows.
cumulative_eval
arg_where- Get indices where condition holds.
arg_where
search_sorted- Find indices where elements should be inserted to maintain order.
search_sorted
offset_by- Add an offset to dates that take months and leap years into account.
offset_by
trigonometry- Trigonometric functions.
trigonometry
sign- Compute the element-wise sign of a series.
sign
propagate_nans-NaN-propagating min/max aggregations.
propagate_nans
NaN
Dataframe pretty printing:fmt- Activate dataframe formatting.
fmt- Activate dataframe formatting.
fmt
Only needed if you are on Windows.↩
Only needed if you are on Windows.↩

